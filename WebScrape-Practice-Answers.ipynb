{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping: Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful link: \n",
    "- https://medium.freecodecamp.org/how-to-scrape-websites-with-python-and-beautifulsoup-5946935d93fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of San Diego Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# San Diego Communities Webpage\n",
    "sd_communities = 'https://en.wikipedia.org/wiki/List_of_communities_and_neighborhoods_of_San_Diego'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Setup: First, scrape the web page above, and use BeautifulSoup to parse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "page = requests.get(sd_communities)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of communities and neighborhoods of San Diego - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "# What is the title of the webpage?\n",
    "\n",
    "# YOUR CODE HERE\n",
    "title = soup.title.string\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: we would like a dictionary of all the communities listed in the wikipedia page, with their links. \n",
    "\n",
    "We want all the community names as keys, and the (relative) links as values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'University Heights': '/wiki/University_Heights/', 'La Jolla': '/wiki/La Jolla/'}\n"
     ]
    }
   ],
   "source": [
    "# It should look something like this:\n",
    "example = {\n",
    "    'University Heights' : '/wiki/University_Heights/',\n",
    "    'La Jolla' : '/wiki/La Jolla/'\n",
    "}\n",
    "\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communities - Part 1\n",
    "\n",
    "Create a dictionary called `communities`, and fill it with the communities information, as above. \n",
    "\n",
    "From your `soup` object, use the find_all method to find all the links. \n",
    "\n",
    "Using that, you can loop through all links, to collect them into a dictionary.\n",
    "\n",
    "For a first pass, don't worry about sub-selecting links, just get all links on the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOU CODE HERE\n",
    "communities = dict()\n",
    "for link in soup.find_all('a'):\n",
    "    title = link.get('title')\n",
    "    link = link.get('href')\n",
    "    communities[title] = link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the resulting dictionary\n",
    "#communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communities - Part 2\n",
    "\n",
    "If you did the part above, extracting links, you probably realized that you extracted a whole bunch of links you don't really want, for example, links from the side bar.\n",
    "\n",
    "Figure out how to sub-select the part of the page that includes the table with all the links, and then run the the same link extraction on that specific part of the page. This should allow you to only extact the relevant links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "table = soup.find('table')\n",
    "\n",
    "communities = dict()\n",
    "for link in table.find_all('a'):\n",
    "    title = link.get('title')\n",
    "    link = link.get('href')\n",
    "    communities[title] = link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check out the results\n",
    "#communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communities - Part 3\n",
    "\n",
    "You now have a dictionary of neighbourhoods in San Diego, and links to their respective pages on wikipedia.\n",
    "\n",
    "See if you can loop through the list of links you have, and collect latitute and longitude data from each one (if available). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First - figure out how to do this with one example page\n",
    "community_page = 'https://en.wikipedia.org/wiki/Ocean_Beach,_San_Diego'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the page\n",
    "page = requests.get(community_page)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the infobox table\n",
    "table = soup.select_one(\"table.infobox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract lat & lon from the table\n",
    "lat = table.select_one(\"span.latitude\").contents[0]\n",
    "lon = table.select_one(\"span.longitude\").contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop across all community pages\n",
    "base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "lats, lons = {}, {}\n",
    "for name, link in communities.items():\n",
    "    url = base_url + link\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    try:\n",
    "        table = soup.select_one(\"table.infobox\")\n",
    "        lat = table.select(\"span.latitude\")[0].contents[0]\n",
    "        lon = table.select(\"span.longitude\")[0].contents[0]\n",
    "    except:\n",
    "        lat, lon = None, None\n",
    "    \n",
    "    lats[name] = lat\n",
    "    lons[name] = lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check out the results\n",
    "#lats\n",
    "#lons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## San Diego Crime Stats Page\n",
    "\n",
    "Check out the San Diego crime stats page below. Let's try and get some data from it. \n",
    "\n",
    "From the landing page, pull all all the table data, storing it into a dictionary that encodes the type of crime, and the number. \n",
    "\n",
    "Hints:\n",
    "- Look for the HTML tag that holds the table data, and loop through all of those labels. \n",
    "- Using this approach, you can get all the table data by looping across one tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SD Crime stats page\n",
    "crime_stats_link = \"http://crimestats.arjis.org/default.aspx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = requests.get(crime_stats_link)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat = {}\n",
    "for tag in soup.find_all('nobr'):\n",
    "    \n",
    "    # Check if value is a label\n",
    "    if tag.contents[0][0].isalpha():\n",
    "        label = tag.contents[0]\n",
    "        dat[label] = 0\n",
    "\n",
    "    # If its not a label (then it's a number) - add to the most recent label\n",
    "    else:\n",
    "        dat[label] = tag.contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Aggravated Assault**': '156',\n",
       " 'Armed Robbery': '26',\n",
       " 'Crime Index Total**': '1029',\n",
       " 'Motor Vehicle Theft': '133',\n",
       " 'Murder': '3',\n",
       " 'Non-Residential Burglary': '51',\n",
       " 'Rape**': '14',\n",
       " 'Residential Burglary': '89',\n",
       " 'Strong Arm Robbery': '24',\n",
       " 'Theft < $400': '332',\n",
       " 'Theft >= $400': '201',\n",
       " 'Total Burglary': '140',\n",
       " 'Total Property Crime': '806',\n",
       " 'Total Thefts': '533',\n",
       " 'Total Violent Crime**': '223'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The crime page above takes inputs to select dates and places. How could we programmatically enter queries into it, and get the results?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
